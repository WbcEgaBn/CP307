{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76e89876-a466-44fa-bac4-9d0538262ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e8d9d63-ca86-45a3-a151-3cdce0b63d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Infomap():\n",
    "    \n",
    "    # initialization of the infomap where G is a nx array and weight is a boolean\n",
    "    \n",
    "    def __init__(self, G, weight):\n",
    "        self.G = G\n",
    "        self.weight = bool(weight)\n",
    "        self.nodes = list(self.G.nodes())\n",
    "        self.edges = list(self.G.edges())\n",
    "        self.sz = len(self.nodes)\n",
    "\n",
    "\n",
    "    # get the total edge weight for the graph\n",
    "    # unweighted: returns number of edges\n",
    "    # weighted: returns sun of edge weights\n",
    "    \n",
    "    def get_edge_weights(self):\n",
    "        if self.weight == True:\n",
    "            total_weight = 0.0\n",
    "            for i, j, k in self.G.edges(data=True):\n",
    "                total_weight += float(k.get(\"weight\", 1.0)) # get the weight val of an edge (from edge dict), default to 1 if it does not exist\n",
    "            return total_weight\n",
    "        else:\n",
    "            return float(self.G.number_of_edges())\n",
    "\n",
    "\n",
    "    # get stationary probalility for moving between nodes in a random walk\n",
    "    # i.e. fraction of time spent at this node\n",
    "    # uw: pi[n] = degree[n] / (2m)    - number of edges connected to n divided bu 2 times total num edges\n",
    "    # w: pi[n] = strength[n] / (2w)   - sum of weights connected to n divided by 2 times total edge weights\n",
    "    \n",
    "    def get_stationary_probability(self):\n",
    "        total = self.get_edge_weights()  # m or w\n",
    "        probabilities = {}\n",
    "    \n",
    "        # if there are no edges return 0\n",
    "        if total == 0:\n",
    "            for n in self.nodes:\n",
    "                probabilities[n] = 0.0\n",
    "            return probabilities\n",
    "    \n",
    "        # if weighted\n",
    "        if self.weight == True:\n",
    "            strengths = dict(self.G.degree(weight=\"weight\"))  # get dict of neighbors\n",
    "            for n in self.nodes:\n",
    "                s = strengths.get(n, 0.0) # get for node n and cast to float\n",
    "                probabilities[n] = float(s) / (2.0 * total)\n",
    "        # if unweighted\n",
    "        else:\n",
    "            degrees = dict(self.G.degree())  # get dict of neighbors\n",
    "            for n in self.nodes:\n",
    "                d = degrees.get(n, 0) # get for node n\n",
    "                probabilities[n] = float(d) / (2.0 * total)\n",
    "                \n",
    "        return probabilities\n",
    "\n",
    "\n",
    "    # get exit probabilities\n",
    "    # qm = weight of edges from u to k / 2 * weight of edges total (divide by 2 so it sums to a prob (1))\n",
    "    # node_communities = dict from node to community id\n",
    "    \n",
    "    def get_exit_probabilities(self, node_communities):\n",
    "        total_weight = self.get_edge_weights()\n",
    "        \n",
    "        # if total weight = 0 return 0\n",
    "        if total_weight == 0:\n",
    "            return {c: 0.0 for c in set(node_communities.values())}\n",
    "    \n",
    "        q_m = {c: 0.0 for c in set(node_communities.values())}\n",
    "    \n",
    "        # loop over edges\n",
    "        # if weighted\n",
    "        if self.weight == True:\n",
    "            for i, j, k in self.G.edges(data=True):\n",
    "                w = float(k.get(\"weight\", 1.0))\n",
    "                # if in diff communities count it, if not, skip\n",
    "                if node_communities[i] != node_communities[j]:\n",
    "                    q_m[node_communities[i]] += w\n",
    "                    q_m[node_communities[j]] += w\n",
    "        # if unweighted\n",
    "        else:\n",
    "            for i, j in self.G.edges():\n",
    "                # if in diff communities count it, if not, skip\n",
    "                if node_communities[i] != node_communities[j]:\n",
    "                    q_m[node_communities[i]] += 1.0\n",
    "                    q_m[node_communities[j]] += 1.0\n",
    "    \n",
    "        # normalize so it sums to a prob\n",
    "        for c in q_m:\n",
    "            q_m[c] /= (2.0 * total_weight)\n",
    "    \n",
    "        return q_m\n",
    "\n",
    "\n",
    "    # map final communities to a dictionary to return\n",
    "    \n",
    "    def map(self, node_comms):\n",
    "        communities = {}\n",
    "        for n, c in node_comms.items():\n",
    "            if c not in communities:\n",
    "                communities[c] = []\n",
    "            communities[c].append(n)\n",
    "\n",
    "        # fix so that community keys are in the correct order\n",
    "        ordered = sorted(communities.keys())\n",
    "        remap = {com: i for i, com in enumerate(ordered)}\n",
    "        newindex = {remap[i]: communities[i] for i in ordered}\n",
    "        return newindex\n",
    "\n",
    "\n",
    "    # compute l: the per-step description length for module partition M. \n",
    "    # that is, for module partition M of n nodes into m modules\n",
    "    # L = q * H(Q) + sum from m=1 to n of m pdot m * H(p of m)\n",
    "    # q = sum q_m (the total probability that the random walker enters any of the m modules)\n",
    "    # pdot m = q_m + sum for i in m of pi[i] (which is given by the total\n",
    "    # probability that any node in the module is visited, plus the probability that the\n",
    "    # random walker exits the module and the exit codeword is used\n",
    "    # H(Q) = q_m / q (The frequency-weighted average length of codewords in the index codebook)\n",
    "    # H(p of m) = q_m and pi[i] for in in m over pdot m (The entropy of the relative rates at which \n",
    "    # the random walker exits module i and visits each node in module i\n",
    "    \n",
    "    def compute_l(self, partition, edge_weights=None, pi=None, q_m=None):\n",
    "\n",
    "        # set initial values\n",
    "        if edge_weights is None:\n",
    "            edge_weights = self.get_edge_weights()\n",
    "    \n",
    "        if pi is None:\n",
    "            pi = self.get_stationary_probability()\n",
    "    \n",
    "        if q_m is None:\n",
    "            q_m = self.get_exit_probabilities(partition)\n",
    "    \n",
    "        modules = set(partition.values())\n",
    "    \n",
    "        # initialize variables\n",
    "        q = 0.0\n",
    "        p = 0.0\n",
    "        H_Q = 0.0\n",
    "        H_Pm = 0.0\n",
    "    \n",
    "        # compute q\n",
    "        for m in modules:\n",
    "            q += q_m.get(m, 0.0)\n",
    "    \n",
    "        # compute H(Q)\n",
    "        if q > 0.0:\n",
    "            H_Q = -sum(\n",
    "            (q_m[m]/q) * math.log2(q_m[m]/q)\n",
    "            for m in modules\n",
    "            if q_m[m] > 0.0\n",
    "        )\n",
    "    \n",
    "        # compute H(p of m)\n",
    "        for m in modules:\n",
    "            # compute p dot m (how often we are in community m)\n",
    "            prob_of_visit = []\n",
    "            for i in self.nodes:\n",
    "                if partition[i] == m:\n",
    "                    prob_of_visit.append(pi[i])\n",
    "    \n",
    "            prob_of_exit = q_m[m]\n",
    "            pdot = sum(prob_of_visit) + prob_of_exit\n",
    "    \n",
    "            if pdot > 0:\n",
    "                probs = []\n",
    "                probs.append(prob_of_exit / pdot)\n",
    "    \n",
    "                for i, comm in partition.items():\n",
    "                    if comm == m:\n",
    "                        probs.append(pi[i] / pdot)\n",
    "    \n",
    "                H_Pm += pdot * (-sum(p * math.log2(p) for p in probs if p > 0))\n",
    "    \n",
    "        return q * H_Q + H_Pm\n",
    "    \n",
    "\n",
    "    # run infomap\n",
    "    \n",
    "    def run(self):\n",
    "        # start timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # start memory tracking\n",
    "        tracemalloc.start()\n",
    "\n",
    "        # build starting communities\n",
    "        communities = {n: i for i, n in enumerate(self.nodes)}\n",
    "        pi = self.get_stationary_probability()\n",
    "\n",
    "        # compute initial L\n",
    "        L = self.compute_l(communities, pi=pi)\n",
    "\n",
    "        # number of iterations (can change as needed)\n",
    "        for i in range(10):\n",
    "    \n",
    "            # randomize order of nodes\n",
    "            node_list = list(self.nodes)\n",
    "            random.shuffle(node_list)\n",
    "    \n",
    "            for n in node_list:\n",
    "                # get the current community of n\n",
    "                current = communities[n]\n",
    "    \n",
    "                # find the community ids of n's neighbors\n",
    "                neighbor_comms = set()\n",
    "                for ni in self.G.neighbors(n):\n",
    "                    neighbor_comms.add(communities[ni])\n",
    "    \n",
    "                # initialize tracking metrics\n",
    "                current_L = L\n",
    "                new_current = current\n",
    "                best_L = current_L\n",
    "\n",
    "                # move n into the community of its neighbors\n",
    "                for node in neighbor_comms:\n",
    "                    communities[n] = node\n",
    "                    # compute new L for node\n",
    "                    new_L = self.compute_l(communities, pi=pi)\n",
    "\n",
    "                    # if the new description length is lower we update\n",
    "                    # the best description length\n",
    "                    if new_L < best_L:\n",
    "                        best_L = new_L\n",
    "                        new_current = node\n",
    "\n",
    "                # if we have found a better move\n",
    "                if new_current != current:\n",
    "                    # move the node into that community and update L\n",
    "                    communities[n] = new_current\n",
    "                    L = best_L\n",
    "                else:\n",
    "                    communities[n] = current\n",
    "\n",
    "        # map return values to the correct format\n",
    "        retval = self.map(communities)\n",
    "\n",
    "        # end timer\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "\n",
    "        # end memory tracking\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "\n",
    "        # convert units\n",
    "        memory_used_kb = peak / 1024\n",
    "        \n",
    "        return retval, total_time, memory_used_kb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
